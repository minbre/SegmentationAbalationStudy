{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "#from imutils import paths #Python library that provides a collection of convenience functions for common tasks in computer vision and image processing\n",
    "#from sklearn.model_selection import train_test_split  #An open-source Python library that provides a wide range of tools and algorithms for machine learning, data mining, and data analysis.\n",
    "import json\n",
    "import importlib\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn #All neural network modules, nn.linear, nn.conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim #For all optimization algorithms SGD, Adam\n",
    "import torch.nn.functional as F #All functions that dont have any parameter\n",
    "from torch.utils.data import Dataset, DataLoader #Pytorch standard dataset and its management\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Pytorych Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118 True\n"
     ]
    }
   ],
   "source": [
    "#Check Pytorch Installation\n",
    "print(torch.version.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Loading the images and masks test dataset\n",
    "test_dataset_root_directory = os.path.join(os.environ[\"HOME\"], \"Segmentation_Project/Crack_Datasets/Crack500_cropped/Test\") \n",
    "\n",
    "test_images_dataset_path = os.path.join(test_dataset_root_directory, \"Images\") # provide the test images path\n",
    "test_masks_dataset_path = os.path.join(test_dataset_root_directory, \"Masks\")    # provide the test masks path\n",
    "\n",
    "test_images_filenames = list(sorted(os.listdir(test_images_dataset_path)))\n",
    "print(\"Testing dataset size:\", len(test_images_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pixel values in train mask: [  0. 255.]\n",
      "Unique pixel values in train mask: [  0. 255.]\n",
      "Unique pixel values in train mask: [  0. 255.]\n",
      "Unique pixel values in train mask: [  0. 255.]\n",
      "Unique pixel values in train mask: [  0. 255.]\n"
     ]
    }
   ],
   "source": [
    "#Check the pixel values of the mask image:\n",
    "for i, image_filename in enumerate(test_images_filenames[:5]):\n",
    "        \n",
    "        test_mask_path = os.path.join(test_masks_dataset_path, image_filename.replace(\".jpg\", \".png\")) #mask path\n",
    "        test_mask = np.array(Image.open(test_mask_path).convert(\"L\"), dtype=np.float32) #open masks from the mask path specified\n",
    "        print(\"Unique pixel values in train mask:\", np.unique(test_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pytorch Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationInferenceDataset(Dataset):\n",
    "    def __init__(self, images_filename, images_directory, masks_directory, transform=None):\n",
    "        # store the images filenames, images path, masks path, augmentations transform\n",
    "        self.images_filenames = images_filename\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of total samples contained in the dataset\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image_path = os.path.join(self.images_directory, image_filename)  # provide the image path\n",
    "        mask_path = os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")) # provide the mask  path\n",
    "\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\")) #open images from image path provided (PIL format required by Pytorch)\n",
    "        #original_size = tuple(image.shape[:2]) #we need original sizes if we resize the input image\n",
    "\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) #open masks from the mask path provided (PIL format required by Pytorch)\n",
    "        #Preprocess mask\n",
    "        mask[mask == 255.0] = 1.0 # for binary segmentation convert the 255 pixel values to 1, sigmoid[0-1] as last activation layer\n",
    "\n",
    "        # check to see if we are applying any transformations\n",
    "        if self.transform is not None:\n",
    "           augmentations = self.transform(image=image, mask=mask)\n",
    "           image = augmentations[\"image\"] # apply the transformations to image\n",
    "           mask = augmentations[\"mask\"] #apply the transformation to mask\n",
    "\n",
    "        # return a tuple of images (with original size if resize) and masks\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        #A.Resize(height = 288, width = 512, interpolation=cv2.INTER_NEAREST), \n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0], \n",
    "            std = [1.0,1.0,1.0],\n",
    "            max_pixelvalues=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "test_dataset = SegmentationInferenceDataset(test_images_filenames, test_images_dataset_path, test_masks_dataset_path, transform=test_transform,)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def dice_score(pred, target, threshold, eps=1e-15): # --> main metric to monitor for validation (it corresponds to F-1 score)\n",
    "    pred = (pred > threshold).float() #binarize predictions based on the given threshold\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice_score = (2. * intersection + eps) / (union + eps)\n",
    "    return dice_score\n",
    "\n",
    "def Jaccard_score(pred, target, threshold, eps=1e-15):\n",
    "    pred = (pred > threshold).float() #binarize predictions based on the given threshold\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    Jaccard_score = (intersection + eps) / (union + eps)\n",
    "    return Jaccard_score\n",
    "\n",
    "def IOU_score(pred, target, threshold):\n",
    "    pred = (pred > threshold).float() #binarize predictions based on the given threshold\n",
    "    tp = ((pred == 1) & (target == 1)).sum() # True Positives (TP): Both pred and target are 1\n",
    "    fp = ((pred == 1) & (target == 0)).sum() # False Positives (FP): pred is 1, target is 0\n",
    "    fn = ((pred == 0) & (target == 1)).sum()  # False Negatives (FN): pred is 0, target is 1\n",
    "    IOU_score = tp / (tp + fp + fn)\n",
    "    return IOU_score\n",
    "\n",
    "def accuracy(pred, target, threshold):\n",
    "    pred = (pred > threshold).float() #binarize predictions based on the given threshold\n",
    "    num_correct = (pred == target).sum()\n",
    "    num_pixels = torch.numel(target)\n",
    "    accuracy = num_correct/num_pixels\n",
    "    return accuracy*100\n",
    "\n",
    "def precision(pred, target, threshold, eps=1e-15):\n",
    "    pred = (pred > threshold).float() #binarize predictions based on the given threshold\n",
    "    tp = ((pred == 1) & (target == 1)).sum() # True Positives (TP): Both pred and target are 1\n",
    "    fp = ((pred == 1) & (target == 0)).sum() # False Positives (FP): pred is 1, target is 0\n",
    "    precision = (tp + eps) / (tp + fp + eps) # eps (very small) to avoid nan (division by zero as observed for some models)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_dataset, params):\n",
    "    \n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_running_accuracy = 0\n",
    "    test_running_precision = 0\n",
    "    test_running_dice_score = 0\n",
    "    test_running_IOU_score = 0\n",
    "    test_dataset_steps = len(test_dataset) // params[\"batch_size\"]\n",
    "\n",
    "    inference_output = []\n",
    "\n",
    "    model.eval () \n",
    "    #with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(test_data_loader, start=1):\n",
    "\n",
    "        #print(f\"Processing batch: {batch_idx}\") #print the processing batch to keep track\n",
    "            \n",
    "\n",
    "        images = images.to(params[\"device\"], non_blocking =True) #load the images in a batch and move to gpu for computation\n",
    "        targets = targets.float().unsqueeze(1).to(params[\"device\"], non_blocking=True) #load the targets mask, add the batch dimension, convert to float point and move it to GPU for computation\n",
    "\n",
    "        # Extract original dimensions from `original_sizes` if resize the input image and mask\n",
    "        #original_heights, original_widths = original_sizes\n",
    "        #print(f\"Original_heights: {original_heights}\") #should be a tensor of batch size\n",
    "        #print(f\"Original_widths: {original_widths}\") #should be a tensor of batch size\n",
    "\n",
    "        #forward propagation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if params[\"model\"] == \"DcsNet_deep\":\n",
    "                predictions, deep_sup_1, deep_sup_2 = model(images)\n",
    "            else:\n",
    "                predictions = model(images) #make predictions on the batch of test images\n",
    "        \n",
    "            prediction_probabilities = torch.sigmoid(predictions.squeeze(1)) #converting the predictions to probability and removing the channel dimension as gray scale\n",
    "\n",
    "            #After making the predictions calculate the accuracy, precision, dice_score and IOU_score as evaluation metrics for the current batch of the test images\n",
    "            accuracy_value = accuracy(torch.sigmoid(predictions), targets, params[\"threshold\"])\n",
    "            precision_value = precision(torch.sigmoid(predictions), targets, params[\"threshold\"])\n",
    "            dice_score_value = dice_score(torch.sigmoid(predictions), targets, params[\"threshold\"])\n",
    "            IOU_score_value = IOU_score(torch.sigmoid(predictions), targets, params[\"threshold\"])\n",
    "\n",
    "            #update the evaluation metrics for each batch\n",
    "            test_running_accuracy += accuracy_value.item()\n",
    "            test_running_precision += precision_value.item()\n",
    "            test_running_dice_score += dice_score_value.item()\n",
    "            test_running_IOU_score += IOU_score_value.item()\n",
    "\n",
    "            #store the predictions with original_height and original width (if resize the image and mask) for visualization\n",
    "            predicted_masks = (prediction_probabilities > params[\"threshold\"]).float() * 1.0 # Binary thresholding\n",
    "            predicted_masks = predicted_masks.cpu().numpy()  # Convert to NumPy array and shift to cpu memory\n",
    "            \n",
    "            # Append predictions with correct handling\n",
    "            for predicted_mask in predicted_masks:\n",
    "                \n",
    "                #print(f\"Predicted_mask_size: {predicted_mask.shape}\")\n",
    "                \n",
    "                inference_output.append((predicted_mask))  # Correctly append to list\n",
    "                \n",
    "\n",
    "    #calculate the average metrics           \n",
    "    test_accuracy = test_running_accuracy / test_dataset_steps #average test accuracy for the entire test dataset\n",
    "    test_precision = test_running_precision / test_dataset_steps #average test precision for the entire test dataset\n",
    "    test_dice_score = test_running_dice_score / test_dataset_steps #average test dice score for the entire test dataset\n",
    "    test_IOU_score = test_running_IOU_score / test_dataset_steps #average test jaccard score for the entire test dataset\n",
    "    \n",
    "    #Create a dictionary for evaluation metrics\n",
    "    eval_metrics = {\n",
    "        \"accuracy\": test_accuracy,\n",
    "        \"precision\": test_precision,\n",
    "        \"dice_score\": test_dice_score,\n",
    "        \"IOU_score\": test_IOU_score\n",
    "    }\n",
    "\n",
    "    print(\"Test Accuracy: {:.4f}, Test Precision: {:.4f}, Test Dice Score: {:.4f}, Test IOU Score: {:.4f}\".format(test_accuracy, test_precision, test_dice_score, test_IOU_score))\n",
    "\n",
    "    return eval_metrics, inference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load the trained models and do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory to save trained models\n",
    "model_save_root_directory = \"/home/ali/Segmentation_Project/Outputs/Trained_Models\"\n",
    "\n",
    "def load_model_and_inference(model_name, test_dataset, test_params):\n",
    "\n",
    "    #load the trained model\n",
    "    trained_model = torch.load(os.path.join(model_save_root_directory, f\"{model_name}\")).to(test_params[\"device\"]) #load the model and transfer it to gpu\n",
    "    print(f\"Trained {model_name} model is loaded successfully...\")\n",
    "\n",
    "    #make inference using the trained model\n",
    "    eval_metrics, inference_output = inference(trained_model, test_dataset, test_params)\n",
    "\n",
    "    return eval_metrics, inference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference ResNet-UNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/miniconda3/envs/seg_prac/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ResUNet18.pth model is loaded successfully...\n",
      "Test Accuracy: 95.6144, Test Precision: 0.7690, Test Dice Score: 0.7269, Test IOU Score: 0.5805\n",
      "Trained ResUNet18_AM.pth model is loaded successfully...\n",
      "Test Accuracy: 95.6436, Test Precision: 0.7639, Test Dice Score: 0.7324, Test IOU Score: 0.5869\n",
      "Trained ResUNet34_2.pth model is loaded successfully...\n",
      "Test Accuracy: 95.6957, Test Precision: 0.7604, Test Dice Score: 0.7383, Test IOU Score: 0.5939\n",
      "Trained ResUNet34_AM.pth model is loaded successfully...\n",
      "Test Accuracy: 95.7251, Test Precision: 0.7619, Test Dice Score: 0.7400, Test IOU Score: 0.5960\n"
     ]
    }
   ],
   "source": [
    "#Define test parameters\n",
    "params = {\n",
    "    \"model\": \"ResNetUNet\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 16, #same as used for training and validation\n",
    "    \"num_workers\": 2, #same as used for training and validation\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "metric_ResNetUNet18, infer_ResNetUNet18 = load_model_and_inference(\"ResUNet18.pth\", test_dataset, params) #ResUNet18_2 model\n",
    "metric_ResNetUNet18_AM, infer_ResNetUNet18_AM = load_model_and_inference(\"ResUNet18_AM.pth\", test_dataset, params) #ResUNet18_AM model\n",
    "metric_ResNetUNet34, infer_ResNetUNet34 = load_model_and_inference(\"ResUNet34_2.pth\", test_dataset, params) #ResUNet34 model\n",
    "metric_ResNetUNet34_AM, infer_ResNetUNet34_AM = load_model_and_inference(\"ResUNet34_AM.pth\", test_dataset, params) #ResUNet34_AM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference EfficientNet-UNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained EffNetUNetB1_AM_2.pth model is loaded successfully...\n",
      "Test Accuracy: 95.5134, Test Precision: 0.7617, Test Dice Score: 0.7242, Test IOU Score: 0.5766\n",
      "Trained EffNetUNetB2_AM.pth model is loaded successfully...\n",
      "Test Accuracy: 95.4996, Test Precision: 0.7653, Test Dice Score: 0.7222, Test IOU Score: 0.5740\n",
      "Trained EffNetUNetB3_AM_2.pth model is loaded successfully...\n",
      "Test Accuracy: 95.5243, Test Precision: 0.7570, Test Dice Score: 0.7278, Test IOU Score: 0.5805\n",
      "Trained EffNetUNetB4_AM_2.pth model is loaded successfully...\n",
      "Test Accuracy: 95.4210, Test Precision: 0.7428, Test Dice Score: 0.7262, Test IOU Score: 0.5787\n"
     ]
    }
   ],
   "source": [
    "#Define test parameters\n",
    "params = {\n",
    "    \"model\": \"EfficientNetUNet\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 16, #same as used for training and validation\n",
    "    \"num_workers\": 2, #same as used for training and validation\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "#metric_EffNetUNetB1, infer_EffNetUNetB1 = load_model_and_inference(\"EffNetUNetB1_2.pth\", test_dataset, params) #EffNetUNetB1 model\n",
    "metric_EffNetUNetB1_AM, infer_EffNetUNetB1_AM = load_model_and_inference(\"EffNetUNetB1_AM_2.pth\", test_dataset, params) #EffNetUNetB1_AM model\n",
    "#metric_EffNetUNetB2, infer_EffNetUNetB2 = load_model_and_inference(\"EffNetUNetB2.pth\", test_dataset, params) #EffNetUNetB2 model\n",
    "metric_EffNetUNetB2_AM, infer_EffNetUNetB2_AM = load_model_and_inference(\"EffNetUNetB2_AM.pth\", test_dataset, params) #EffNetUNetB2_AM model\n",
    "#metric_EffNetUNetB3, infer_EffNetUNetB3 = load_model_and_inference(\"EffNetUNetB3_2.pth\", test_dataset, params) #EffNetUNetB3 model\n",
    "metric_EffNetUNetB3_AM, infer_EffNetUNetB3_AM = load_model_and_inference(\"EffNetUNetB3_AM_2.pth\", test_dataset, params) #EffNetUNetB3_AM model\n",
    "#metric_EffNetUNetB4, infer_EffNetUNetB4 = load_model_and_inference(\"EffNetUNetB4_2.pth\", test_dataset, params) #EffNetUNetB3 model\n",
    "metric_EffNetUNetB4_AM, infer_EffNetUNetB4_AM = load_model_and_inference(\"EffNetUNetB4_AM_2.pth\", test_dataset, params) #EffNetUNetB3_AM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference MANet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained ResMANet18.pth model is loaded successfully...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.5892, Test Precision: 0.7644, Test Dice Score: 0.7284, Test IOU Score: 0.5814\n",
      "Trained ResMANet34.pth model is loaded successfully...\n",
      "Test Accuracy: 95.7076, Test Precision: 0.7638, Test Dice Score: 0.7374, Test IOU Score: 0.5927\n"
     ]
    }
   ],
   "source": [
    "#Define test parameters\n",
    "params = {\n",
    "    \"model\": \"ResNetMANet\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 16, #same as used for training and validation\n",
    "    \"num_workers\": 2, #same as used for training and validation\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "metric_ResMANet18, infer_ResMANet18 = load_model_and_inference(\"ResMANet18.pth\", test_dataset, params) #ResNetMANet model\n",
    "metric_ResMANet34, infer_ResMANet34 = load_model_and_inference(\"ResMANet34.pth\", test_dataset, params) #ResNetMANet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference DcsNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained DcsNet_2.pth model is loaded successfully...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.7153, Test Precision: 0.7737, Test Dice Score: 0.7337, Test IOU Score: 0.5879\n"
     ]
    }
   ],
   "source": [
    "#Define test parameters\n",
    "params = {\n",
    "    \"model\": \"DcsNet\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 16, #same as used for training and validation\n",
    "    \"num_workers\": 2, #same as used for training and validation\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "metric_DcsNet, infer_DcsNet = load_model_and_inference(\"DcsNet_2.pth\", test_dataset, params) #DcsNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference UTNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained UTNet.pth model is loaded successfully...\n",
      "Test Accuracy: 94.7537, Test Precision: 0.2631, Test Dice Score: 0.3384, Test IOU Score: 0.2336\n"
     ]
    }
   ],
   "source": [
    "#Define test parameters\n",
    "params = {\n",
    "    \"model\": \"UTNet\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 16, #same as used for training and validation\n",
    "    \"num_workers\": 2, #same as used for training and validation\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "metric_UTNet, infer_UTNet = load_model_and_inference(\"UTNet.pth\", test_dataset, params) #DcsNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Visualization of Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the mask for better visualization\n",
    "def normalize_array(array):\n",
    "    array_min = array.min()\n",
    "    array_max = array.max()\n",
    "    normalized_array = (array - array_min) / (array_max - array_min) * 255\n",
    "    return normalized_array.astype(np.uint8)\n",
    "\n",
    "def display_inference_image_mask(images_filenames, images_dataset_path, masks_data_path, inference_masks, inference_title, font_size=10):\n",
    "    # Determine the number of columns and rows and adjust figsize accordingly\n",
    "    cols = len(inference_models) + 2  # 2 is added to include the ground truth image and mask along with the predicted masks of trained models\n",
    "    rows = len(images_filenames)\n",
    "    # Set the figsize dynamically based on the number of columns\n",
    "    base_figsize_width = 10  # Base width for three columns\n",
    "    base_figsize_height = 24  # Base height for the given number of rows\n",
    "    extra_width_per_col = 2  # Additional width for each extra column\n",
    "    # Calculate the appropriate figsize based on the number of columns\n",
    "    figsize_width = base_figsize_width + (cols - 3) * extra_width_per_col\n",
    "    figsize = (figsize_width, base_figsize_height)\n",
    "\n",
    "    # Create the subplot grid with the dynamic figsize\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n",
    "\n",
    "    # Loop through the image filenames and display them along with their masks\n",
    "    for i, image_filename in enumerate(images_filenames):\n",
    "        # Load the grounth truth image\n",
    "        image_path = os.path.join(images_dataset_path, image_filename)\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "        # Load the grounth truth mask\n",
    "        mask_path = os.path.join(masks_data_path, image_filename.replace(\".jpg\", \".png\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "        # Display the grounth image and mask\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, cmap='gray')\n",
    "        # Set the titles with the specified font size\n",
    "        ax[i, 0].set_title(\"Image\", fontsize=font_size)\n",
    "        ax[i, 1].set_title(\"GT Mask\", fontsize=font_size)\n",
    "        # Turn off axes\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "\n",
    "        for j, (infer_model, infer_mask) in enumerate(inference_masks.item()):\n",
    "            predicted_mask = normalize_array(inference_masks[infer_model][i])  # Normalize for better color mapping\n",
    "            # Display the predicted mask\n",
    "            ax[i, j+2].imshow(predicted_mask, cmap='gray')\n",
    "            # Set titles of predicted masks\n",
    "            ax[i, j+2].set_title(inference_title[infer_model], fontsize=font_size)\n",
    "            # Turn off axes\n",
    "            ax[i, j+2].set_axis_off()\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()  # Adjust spacing to avoid overlaps\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the list of indices for grounth images and mask\n",
    "selected_indices = [9, 11, 18, 48, 250, 500]\n",
    "selected_GTimages = [test_images_filenames[i] for i in selected_indices]\n",
    "\n",
    "# Provide the dictionary of the trained inference models for the visualization\n",
    "inference_models = {\"ResUNet\": [infer_ResNetUNet18_AM[i] for i in selected_indices],\n",
    "                    \"EffNetUNet\": [infer_EffNetUNetB1_AM[i] for i in selected_indices],\n",
    "                    \"MANet\": [infer_ResMANet34[i] for i in selected_indices],\n",
    "                    \"DcsNet\": [infer_DcsNet[i] for i in selected_indices]\n",
    "                    }\n",
    "# Provide titles of the inference masks from the trained models\n",
    "inference_title = {\"ResUNet\": \"ResNet34-UNet\",\n",
    "                    \"EffNetUNet\": \"EfficientNetB1-UNet\",\n",
    "                    \"MANet\": \"ResNet34-MANet\",\n",
    "                    \"DcsNet\": \"DcsNet\"\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m selected_indices]\n\u001b[1;32m      6\u001b[0m inference_models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResUNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: dummy_inference(),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffNetUNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: dummy_inference(),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMANet\u001b[39m\u001b[38;5;124m\"\u001b[39m: dummy_inference(),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDcsNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: dummy_inference()\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m(\u001b[49m\u001b[43minference_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResUNet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dummy_inference():\n",
    "    return [np.random.rand(1, 256, 256) for _ in selected_indices]\n",
    "\n",
    "inference_models = {\n",
    "    \"ResUNet\": dummy_inference(),\n",
    "    \"EffNetUNet\": dummy_inference(),\n",
    "    \"MANet\": dummy_inference(),\n",
    "    \"DcsNet\": dummy_inference()\n",
    "}\n",
    "\n",
    "print((inference_models[\"ResUNet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'str'>\n",
      "1\n",
      "6\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'str'>\n",
      "2\n",
      "6\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'str'>\n",
      "3\n",
      "6\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for j, (model_name, predicted_masks) in enumerate(inference_models.items()):\n",
    "    print(j)\n",
    "    print(len(predicted_masks))\n",
    "    print((inference_models[model_name][j]))\n",
    "    print(type(inference_title[model_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
